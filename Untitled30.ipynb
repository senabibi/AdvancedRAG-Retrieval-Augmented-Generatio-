{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LAB 1: TOKEN ESTIMATOR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ========================================\n",
        "# STEP 1: INSTALL NECESSARY PACKAGES\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üì¶ Installing packages...\")\n",
        "!pip install -q streamlit pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ========================================\n",
        "# STEP 2: CREATE THE STREAMLIT APP\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìù Creating application file...\")\n",
        "\n",
        "# Define helper functions\n",
        "def count_words(text):\n",
        "    \"\"\"Count the number of words in the text.\"\"\"\n",
        "    words = text.split()\n",
        "    return len(words)\n",
        "\n",
        "def count_characters(text):\n",
        "    \"\"\"Count the number of characters in the text.\"\"\"\n",
        "    return len(text)\n",
        "\n",
        "def estimate_tokens(text):\n",
        "    \"\"\"Estimate token count (1 token ‚âà 4 characters).\"\"\"\n",
        "    char_count = count_characters(text)\n",
        "    estimated_tokens = char_count / 4\n",
        "    return int(estimated_tokens)\n",
        "\n",
        "def check_character_limit(text, limit=4000):\n",
        "    \"\"\"Check the character limit.\"\"\"\n",
        "    char_count = count_characters(text)\n",
        "    return char_count > limit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the main function\n",
        "def main():\n",
        "    st.set_page_config(page_title=\"Token Estimator\", page_icon=\"üìä\", layout=\"wide\")\n",
        "\n",
        "    st.title(\"üìä Token Estimation Tool\")\n",
        "    st.markdown(\"\"\"\n",
        "    ### Upload and analyze your text file\n",
        "    This tool:\n",
        "    - Calculates word count\n",
        "    - Calculates character count\n",
        "    - Estimates token count (1 token ‚âà 4 characters)\n",
        "    - Warns for 4000+ characters\n",
        "    \"\"\")\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"Choose a text file\", type=['txt', 'md', 'csv'])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        try:\n",
        "            content = uploaded_file.read().decode('utf-8')\n",
        "            st.success(f\"‚úÖ File '{uploaded_file.name}' uploaded successfully!\")\n",
        "\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "\n",
        "            word_count = count_words(content)\n",
        "            char_count = count_characters(content)\n",
        "            token_estimate = estimate_tokens(content)\n",
        "\n",
        "            with col1:\n",
        "                st.metric(label=\"üìù Word Count\", value=word_count)\n",
        "            with col2:\n",
        "                st.metric(label=\"üî§ Character Count\", value=char_count)\n",
        "            with col3:\n",
        "                st.metric(label=\"üéØ Estimated Tokens\", value=token_estimate)\n",
        "\n",
        "            if check_character_limit(content):\n",
        "                st.error(\"‚ö†Ô∏è WARNING: This file exceeds 4000 characters!\")\n",
        "                st.warning(f\"Your file contains {char_count} characters (limit: 4000)\")\n",
        "            else:\n",
        "                st.info(f\"‚úì File is within safe limits ({char_count}/4000 characters)\")\n",
        "\n",
        "            with st.expander(\"üìÑ File Content Preview\"):\n",
        "                st.text_area(\"Content\", content, height=300, disabled=True)\n",
        "\n",
        "            st.markdown(\"---\")\n",
        "            st.subheader(\"üìà Additional Statistics\")\n",
        "\n",
        "            col4, col5, col6 = st.columns(3)\n",
        "\n",
        "            with col4:\n",
        "                avg_word_length = char_count / word_count if word_count > 0 else 0\n",
        "                st.metric(\"Avg Word Length\", f\"{avg_word_length:.2f} chars\")\n",
        "            with col5:\n",
        "                lines = content.count('\\n') + 1\n",
        "                st.metric(\"Line Count\", lines)\n",
        "            with col6:\n",
        "                avg_tokens_per_line = token_estimate / lines if lines > 0 else 0\n",
        "                st.metric(\"Tokens Per Line\", f\"{avg_tokens_per_line:.1f}\")\n",
        "\n",
        "        except UnicodeDecodeError:\n",
        "            st.error(\"‚ùå Error: Could not read file. Please upload a text-based file.\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå An error occurred: {str(e)}\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"\"\"\n",
        "    ### ‚ÑπÔ∏è How it works?\n",
        "\n",
        "    **Token Estimation Formula:**\n",
        "    ```\n",
        "    Estimated Tokens = Character Count / 4\n",
        "    ```\n",
        "\n",
        "    **Note:** This is a simple estimation. Actual token count may vary based on:\n",
        "    - The tokenizer used by the LLM\n",
        "    - Language and character encoding\n",
        "    - Special characters and code blocks\n",
        "    \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write the app to a file\n",
        "app_code = f'''\n",
        "import streamlit as st\n",
        "\n",
        "{count_words.__code__.co_name} = {count_words}\n",
        "{count_characters.__code__.co_name} = {count_characters}\n",
        "{estimate_tokens.__code__.co_name} = {estimate_tokens}\n",
        "{check_character_limit.__code__.co_name} = {check_character_limit}\n",
        "{main.__code__.co_name} = {main}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''\n",
        "\n",
        "with open('lab1_app.py', 'w') as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(\"‚úÖ Application file created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ========================================\n",
        "# STEP 3: NGROK SETUP\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüîß Setting up Ngrok...\")\n",
        "print(\"‚ö†Ô∏è Get your free ngrok token here: https://dashboard.ngrok.com/get-started/your-authtoken\\n\")\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import getpass\n",
        "\n",
        "ngrok_token = getpass.getpass('Enter Ngrok token: ')\n",
        "ngrok.set_auth_token(ngrok_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ========================================\n",
        "# STEP 4: START STREAMLIT IN BACKGROUND\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüöÄ Starting Streamlit...\")\n",
        "\n",
        "import os\n",
        "os.system('streamlit run lab1_app.py --server.port 8501 --server.headless true > /dev/null 2>&1 &')\n",
        "\n",
        "import time\n",
        "time.sleep(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ========================================\n",
        "# STEP 5: CREATE NGROK TUNNEL\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üåê Creating Ngrok tunnel...\\n\")\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n‚úÖ SUCCESS! Your app is ready:\\n\")\n",
        "print(f\"   üîó {public_url}\\n\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüìå IMPORTANT:\")\n",
        "print(\"   1. Click the link above\")\n",
        "print(\"   2. Upload a text file\")\n",
        "print(\"   3. See the results!\")\n",
        "print(\"\\n‚ö†Ô∏è Keep this cell running\")\n",
        "\n",
        "try:\n",
        "    import threading\n",
        "    event = threading.Event()\n",
        "    event.wait()\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nüëã Shutting down...\")\n",
        "    ngrok.disconnect(public_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LAB 2A: SIMPLE RAG - TF-IDF & COSINE SIMILARITY "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ========================================\n",
        "# STEP 1: INSTALL REQUIRED PACKAGES\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üì¶ Installing packages...\")\n",
        "!pip install -q streamlit pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ========================================\n",
        "# STEP 2: CREATE THE STREAMLIT APP\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìù Creating application file...\")\n",
        "\n",
        "# Import required libraries\n",
        "import streamlit as st\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "class SimpleTFIDF:\n",
        "    \"\"\"TF-IDF implementation (without external libraries).\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.documents = []\n",
        "        self.document_names = []\n",
        "        self.vocabulary = set()\n",
        "        self.idf_scores = {}\n",
        "        self.document_vectors = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    def add_document(self, text, name):\n",
        "        \"\"\"Add document to the collection.\"\"\"\n",
        "        self.documents.append(text.lower())\n",
        "        self.document_names.append(name)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Simple tokenization.\"\"\"\n",
        "        for char in '.,!?;:\\\"()[]{}':\n",
        "            text = text.replace(char, ' ')\n",
        "        return text.lower().split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    def calculate_tf(self, document):\n",
        "        \"\"\"Calculate Term Frequency.\"\"\"\n",
        "        tokens = self.tokenize(document)\n",
        "        token_count = len(tokens)\n",
        "        term_counts = Counter(tokens)\n",
        "        tf = {}\n",
        "        for term, count in term_counts.items():\n",
        "            tf[term] = count / token_count if token_count > 0 else 0\n",
        "        return tf\n",
        "\n",
        "    def calculate_idf(self):\n",
        "        \"\"\"Calculate Inverse Document Frequency.\"\"\"\n",
        "        num_documents = len(self.documents)\n",
        "        df = Counter()\n",
        "        for document in self.documents:\n",
        "            unique_terms = set(self.tokenize(document))\n",
        "            for term in unique_terms:\n",
        "                df[term] += 1\n",
        "                self.vocabulary.add(term)\n",
        "        for term in self.vocabulary:\n",
        "            self.idf_scores[term] = math.log(num_documents / (1 + df[term]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    def calculate_tfidf_vector(self, document):\n",
        "        \"\"\"Calculate TF-IDF vector.\"\"\"\n",
        "        tf = self.calculate_tf(document)\n",
        "        tfidf = {}\n",
        "        for term in self.vocabulary:\n",
        "            tf_score = tf.get(term, 0)\n",
        "            idf_score = self.idf_scores.get(term, 0)\n",
        "            tfidf[term] = tf_score * idf_score\n",
        "        return tfidf\n",
        "\n",
        "    def build_vectors(self):\n",
        "        \"\"\"Build TF-IDF vectors for all documents.\"\"\"\n",
        "        self.calculate_idf()\n",
        "        for document in self.documents:\n",
        "            vector = self.calculate_tfidf_vector(document)\n",
        "            self.document_vectors.append(vector)\n",
        "\n",
        "    def cosine_similarity(self, vector1, vector2):\n",
        "        \"\"\"Calculate Cosine Similarity.\"\"\"\n",
        "        dot_product = sum(vector1.get(term, 0) * vector2.get(term, 0) for term in self.vocabulary)\n",
        "        magnitude1 = math.sqrt(sum(value ** 2 for value in vector1.values()))\n",
        "        magnitude2 = math.sqrt(sum(value ** 2 for value in vector2.values()))\n",
        "        if magnitude1 == 0 or magnitude2 == 0:\n",
        "            return 0\n",
        "        return dot_product / (magnitude1 * magnitude2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    def find_most_relevant(self, query):\n",
        "        \"\"\"Find the most relevant documents.\"\"\"\n",
        "        query_vector = self.calculate_tfidf_vector(query.lower())\n",
        "        similarities = []\n",
        "        for i, doc_vector in enumerate(self.document_vectors):\n",
        "            similarity = self.cosine_similarity(query_vector, doc_vector)\n",
        "            similarities.append((self.document_names[i], similarity, i))\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "        return similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    st.set_page_config(page_title=\"Simple RAG\", page_icon=\"üîç\", layout=\"wide\")\n",
        "\n",
        "    st.title(\"üîç Simple RAG: Document Search System\")\n",
        "    st.markdown(\"\"\"\n",
        "    ### Find the most relevant document for your query\n",
        "    This system uses **TF-IDF** and **Cosine Similarity** to find the\n",
        "    best matching document (without using an LLM!).\n",
        "    \"\"\")\n",
        "\n",
        "    if 'tfidf' not in st.session_state:\n",
        "        st.session_state.tfidf = SimpleTFIDF()\n",
        "        st.session_state.documents_processed = False\n",
        "\n",
        "    st.subheader(\"üìÅ Step 1: Upload Documents\")\n",
        "    uploaded_files = st.file_uploader(\"Upload multiple text files\",\n",
        "                                     type=['txt', 'md'], accept_multiple_files=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    if uploaded_files:\n",
        "        if st.button(\"Process Documents\"):\n",
        "            st.session_state.tfidf = SimpleTFIDF()\n",
        "            with st.spinner(\"Processing documents...\"):\n",
        "                for uploaded_file in uploaded_files:\n",
        "                    try:\n",
        "                        content = uploaded_file.read().decode('utf-8')\n",
        "                        st.session_state.tfidf.add_document(content, uploaded_file.name)\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"Error: {uploaded_file.name}: {str(e)}\")\n",
        "                st.session_state.tfidf.build_vectors()\n",
        "                st.session_state.documents_processed = True\n",
        "\n",
        "            st.success(f\"‚úÖ {len(uploaded_files)} documents processed successfully!\")\n",
        "\n",
        "            st.subheader(\"üìä Document Summary\")\n",
        "            for i, name in enumerate(st.session_state.tfidf.document_names):\n",
        "                word_count = len(st.session_state.tfidf.tokenize(\n",
        "                    st.session_state.tfidf.documents[i]))\n",
        "                st.write(f\"- **{name}**: {word_count} words\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    if st.session_state.documents_processed:\n",
        "        st.markdown(\"---\")\n",
        "        st.subheader(\"üîé Step 2: Enter Your Query\")\n",
        "\n",
        "        query = st.text_input(\"What are you looking for?\",\n",
        "                             placeholder=\"Example: Tell me about apples\")\n",
        "\n",
        "        if query:\n",
        "            with st.spinner(\"Analyzing query...\"):\n",
        "                results = st.session_state.tfidf.find_most_relevant(query)\n",
        "\n",
        "            st.subheader(\"üìà Results\")\n",
        "\n",
        "            if results:\n",
        "                most_relevant = results[0]\n",
        "                st.success(f\"üéØ Most Relevant Document: **{most_relevant[0]}**\")\n",
        "                st.metric(\"Similarity Score\", f\"{most_relevant[1]:.4f}\")\n",
        "\n",
        "                doc_index = most_relevant[2]\n",
        "                st.text_area(\"Document Content\",\n",
        "                           st.session_state.tfidf.documents[doc_index], height=200)\n",
        "\n",
        "                st.markdown(\"---\")\n",
        "                st.subheader(\"üìä All Documents (Ranked)\")\n",
        "\n",
        "                for rank, (name, similarity, idx) in enumerate(results, 1):\n",
        "                    with st.expander(f\"#{rank} - {name} (Score: {similarity:.4f})\"):\n",
        "                        st.write(st.session_state.tfidf.documents[idx][:500] + \"...\")\n",
        "\n",
        "                st.markdown(\"---\")\n",
        "                st.subheader(\"üìâ Similarity Scores\")\n",
        "                for name, similarity, idx in results:\n",
        "                    st.progress(similarity, text=f\"{name}: {similarity:.4f}\")\n",
        "    else:\n",
        "        st.info(\"üëÜ Please upload and process documents first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"\"\"\n",
        "    ### ‚ÑπÔ∏è How it Works\n",
        "\n",
        "    **TF-IDF:**\n",
        "    - **TF**: Frequency of the term in the document\n",
        "    - **IDF**: Importance of the term across all documents\n",
        "    - **TF-IDF**: Combined score of the two\n",
        "\n",
        "    **Cosine Similarity:**\n",
        "    ```\n",
        "    similarity = (A ¬∑ B) / (||A|| √ó ||B||)\n",
        "    ```\n",
        "    0 = No similarity, 1 = Identical\n",
        "    \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write the app to a file\n",
        "app_code = f'''\n",
        "import streamlit as st\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "{SimpleTFIDF.__name__} = {SimpleTFIDF}\n",
        "{main.__name__} = {main}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''\n",
        "\n",
        "with open('lab2a_app.py', 'w') as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(\"‚úÖ Application file created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ========================================\n",
        "# STEP 3: NGROK SETUP\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüîß Setting up Ngrok...\")\n",
        "print(\"‚ö†Ô∏è Free token: https://dashboard.ngrok.com/get-started/your-authtoken\\n\")\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import getpass\n",
        "\n",
        "ngrok_token = getpass.getpass('Enter your Ngrok token: ')\n",
        "ngrok.set_auth_token(ngrok_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ========================================\n",
        "# STEP 4: START STREAMLIT\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüöÄ Starting Streamlit...\")\n",
        "\n",
        "import os\n",
        "os.system('streamlit run lab2a_app.py --server.port 8501 --server.headless true > /dev/null 2>&1 &')\n",
        "\n",
        "import time\n",
        "time.sleep(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ========================================\n",
        "# STEP 5: CREATE TUNNEL\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üåê Creating Ngrok tunnel...\\n\")\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n‚úÖ SUCCESS! Your app is ready:\\n    üîó {public_url}\\n\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüìå IMPORTANT:\")\n",
        "print(\"    1. Click the link above\")\n",
        "print(\"    2. Upload multiple text files\")\n",
        "print(\"    3. Click 'Process Documents'\")\n",
        "print(\"    4. Enter a query to find the best match!\")\n",
        "print(\"\\n‚ö†Ô∏è Keep this cell running to maintain the connection\\n\")\n",
        "\n",
        "try:\n",
        "    import threading\n",
        "    event = threading.Event()\n",
        "    event.wait()\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nüëã Shutting down...\")\n",
        "    ngrok.disconnect(public_url)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
